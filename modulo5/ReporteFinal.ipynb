{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introducción a la Ciencia de Datos con Python - Reporte Final\n",
    "\n",
    "## Título\n",
    "Tiempo de atención en sucursales de entidad bancaria local\n",
    "\n",
    "## Integrantes\n",
    "- Damián Augusto Meza Candia\n",
    "- Javier Augusto Meza Candia\n",
    "\n",
    "## Contenido\n",
    "- [Introducción](#Introducción)\n",
    "- [Descripción del dataset](#Descripción-del-dataset)\n",
    "- [Tipo de problema planteado](#Tipo-de-problema-planteado)\n",
    "- [Metodología empleada para resolver el problema](#Metodología-empleada-para-resolver-el-problema)\n",
    "    - [Limpieza de datos](#Limpieza-de-datos)\n",
    "    - [Análisis de datos](#Análisis-de-datos)\n",
    "- [Métricas de desempeño utilizadas](#Métricas-de-desempeño-utilizadas)\n",
    "- [Descripción de los resultados obtenidos](#Descripción-de-los-resultados-obtenidos)\n",
    "- [Referencias](#Referencias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "El problema que queremos resolver se trata de predecir el tiempo de espera y atención de los clientes en las diferentes sucursales de una entidad bancaria local, según el tipo de operación/servicio que desean realizar, de manera a optimizar los recursos y lograr una atención más eficaz dentro de las mismas.\n",
    "\n",
    "Preguntas claves:\n",
    "- ¿El tiempo de espera está directamente relacionado con el tipo de operación a realizar?\n",
    "- ¿El tiempo de espera está relacionado con la variable del día de la semana? ¿O el horario del día?\n",
    "- ¿El tiempo de espera está relacionado con la sucursal de atención?\n",
    "- ¿El tiempo de espera está relacionado con la categoría de cliente?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción del dataset\n",
    "El dataset contiene datos de tickets de atención a clientes presenciales en sucursales de una entidad bancaria local durante el mes de Octubre de 2022.\n",
    "Sobre cada atención en particular se cuenta con los siguientes datos: sucursal, zona de la sucursal, sector dentro de la sucursal, box de atención, tipo de atención, estado de atención, categoría del cliente, identificación del cliente, usuario encargado de la atención, tiempo de espera, tiempo de atención, motivo de cierre del ticket, entre otros.\n",
    "\n",
    "- **Variables:** Código de sucursal, tipo de cola, nro. de atención general, fecha de ingreso a la sucursal, fecha y hora de inicio del llamado, fecha y hora de inicio de atención, fecha y hora de fin de atención, código de caja de atención, nro. de ticket, categoría de cliente, tiempo de espera, tiempo atendido, entre otros.\n",
    "- **Tipos de datos:** Contamos con datos categóricos, datos de fechas y datos numéricos.\n",
    "- **Número de registros:** 69.481 registros\n",
    "- **Datos faltantes:** contamos con datos faltantes en la columna de fecha y hora de inicio de atención o fin de atención, en cuyo caso optamos por ignorar los registros (2.450 registros). También descartamos las columnas sin dato alguno en ninguna fila."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tipo de problema planteado\n",
    "Identificamos que el problema corresponde a un problema de regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Metodología empleada para resolver el problema\n",
    "Llevaremos a cabo mediante el lenguaje de programación Python y las librerías disponibles, los pasos necesarios para extraer la información de los sistemas transaccionales, comprender la situación actual y descubrir patrones que ayuden a la toma de decisiones para mejorar la calidad de la atención a los clientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# read CSV file from the 'data' subdirectory using a relative path\n",
    "data = pd.read_csv('dataset_ticket_hist.csv', index_col=0, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first 5 rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the last 5 rows\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the DataFrame (rows, columns)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminamos las columnas que no se necesitan\n",
    "data.drop('COLHORING', inplace=True, axis=1)\n",
    "data.drop('COLABN', inplace=True, axis=1)\n",
    "data.drop('DESPLAN_CONSU', inplace=True, axis=1)\n",
    "data.drop('MODOPAGO', inplace=True, axis=1)\n",
    "data.drop('TIPCUENTA', inplace=True, axis=1)\n",
    "data.drop('ESTCUENTA', inplace=True, axis=1)\n",
    "data.drop('DESMARMOD', inplace=True, axis=1)\n",
    "data.drop('SITUEQUIP', inplace=True, axis=1)\n",
    "data.drop('MOTREGIS', inplace=True, axis=1)\n",
    "data.drop('MHORAENV', inplace=True, axis=1)\n",
    "data.drop('MHORACONF', inplace=True, axis=1)\n",
    "data.drop('EST_PORT', inplace=True, axis=1)\n",
    "\n",
    "# eliminamos otras columnas que no se necesitan\n",
    "data.drop('NRO_DE_ATENCION_GENERAL', inplace=True, axis=1)\n",
    "data.drop('USRCOD', inplace=True, axis=1)\n",
    "data.drop('BOXCOD', inplace=True, axis=1)\n",
    "data.drop('COLNROTICK', inplace=True, axis=1)\n",
    "data.drop('TICKEST', inplace=True, axis=1)\n",
    "data.drop('USRATENDIO', inplace=True, axis=1)\n",
    "data.drop('CONTATEN', inplace=True, axis=1)\n",
    "data.drop('tiempo2daLinea', inplace=True, axis=1)\n",
    "data.drop('yallamo', inplace=True, axis=1)\n",
    "data.drop('derivado_a', inplace=True, axis=1)\n",
    "data.drop('ticket_inicial', inplace=True, axis=1)\n",
    "data.drop('enviado_a_standby', inplace=True, axis=1)\n",
    "data.drop('COLHORAINITSTANBY', inplace=True, axis=1)\n",
    "data.drop('SERV_INICIAL', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminamos las filas con datos faltantes\n",
    "data = data.dropna(subset=['COLHORAINIAT'])\n",
    "data = data.dropna(subset=['COLHORAFINAT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the DataFrame (rows, columns)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función *apply()* en DataFrame tomará alguna función arbitraria que haya escrito y la aplicará a una *serie* (una sola columna) o DataFrame en todas las filas o columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def todatetime(row):\n",
    "    #La fila es un único objeto de *Series* que es una sola fila indexada por valores de columna.\n",
    "    #Convertimos el tipo de dato en una nueva entrada en la serie.\n",
    "    row['FEC_INGRESO']=pd.to_datetime(row['COLFECING'], format=\"%d/%m/%Y %H:%M:%S\")\n",
    "    row['FEC_INI_ATENCION']=pd.to_datetime(row['COLHORAINIAT'], format=\"%d/%m/%Y %H:%M:%S\")\n",
    "    row['FEC_FIN_ATENCION']=pd.to_datetime(row['COLHORAFINAT'], format=\"%d/%m/%Y %H:%M:%S\")\n",
    "    row['FEC_INI_LLAMADA']=pd.to_datetime(row['COLHORAINILLA'], format=\"%d/%m/%Y %H:%M:%S\")\n",
    "    #Ahora solo retornamos la fila y la función apply() se encargará de fusionarlas de vuelta en un DataFrame\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.apply(todatetime, axis=\"columns\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminamos las viejas columnas que YA no se necesitan\n",
    "data.drop('COLFECING', inplace=True, axis=1)\n",
    "data.drop('COLHORAINIAT', inplace=True, axis=1)\n",
    "data.drop('COLHORAFINAT', inplace=True, axis=1)\n",
    "data.drop('COLHORAINILLA', inplace=True, axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TIEMPO_ESPERA'] = round((data.FEC_INI_ATENCION-data.FEC_INGRESO) / pd.Timedelta(minutes=1), 2)\n",
    "data['TIEMPO_TOTAL_ATENCION'] = round((data.FEC_FIN_ATENCION-data.FEC_INI_ATENCION) / pd.Timedelta(minutes=1), 2)\n",
    "data['TIPO_OPERACION'] = data.TIPCOLACOD.str.slice(3)\n",
    "# Find unique values of a column\n",
    "print(data['TIPO_OPERACION'].unique())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toweekday(row):\n",
    "    #La fila es un único objeto de *Series* que es una sola fila indexada por valores de columna.\n",
    "    #Convertimos el tipo de dato en una nueva entrada en la serie.\n",
    "    row['DIA_SEMANA']=row['FEC_INGRESO'].isoweekday()\n",
    "    row['HORA_DIA']=row['FEC_INGRESO'].hour\n",
    "    row['DIA_MES']=row['FEC_INGRESO'].day\n",
    "    #Ahora solo retornamos la fila y la función apply() se encargará de fusionarlas de vuelta en un DataFrame\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorizamos por dia de la semana\n",
    "data=data.apply(toweekday, axis=\"columns\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Análisis de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizamos las funciones de agregación del tiempo de espera (en minutos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por día de la semana\n",
    "# 1=Lunes, 2=Martes, 3=Miércoles, 4=Jueves, 5=Viernes\n",
    "data.groupby(\"DIA_SEMANA\").agg({\"TIEMPO_ESPERA\":(np.min,np.max,np.average,np.median,np.mean,np.std)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por hora del día\n",
    "data.groupby(\"HORA_DIA\").agg({\"TIEMPO_ESPERA\":(np.min,np.max,np.average,np.median,np.mean,np.std)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recorte\n",
    "El recorte implica la limitación de todos los valores por debajo o por encima de un determinado valor. El recorte es útil cuando una columna contiene algunos valores atípicos. Podemos establecer un valor máximo vmax y un valor mínimo vmin y establecer todos los valores atípicos mayores que el valor máximo en vmax y todos los valores atípicos menores que el valor mínimo en vmin.\n",
    "Establecemos los valores mínimo y máximo para el tiempo de espera y el tiempo total de atención (en minutos):\n",
    "- vmin = 0\n",
    "- vmax = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = 0\n",
    "vmax = 90\n",
    "\n",
    "data['TIEMPO_ESPERA_CLIP'] = data['TIEMPO_ESPERA'].apply(lambda x: vmax if x > vmax else vmin if x < vmin else x)\n",
    "data['TIEMPO_TOTAL_ATENCION_CLIP'] = data['TIEMPO_TOTAL_ATENCION'].apply(lambda x: vmax if x > vmax else vmin if x < vmin else x)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a analizar las funciones de agregación del tiempo de espera (en minutos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por día de la semana\n",
    "# 1=Lunes, 2=Martes, 3=Miércoles, 4=Jueves, 5=Viernes\n",
    "data.groupby(\"DIA_SEMANA\").agg({\"TIEMPO_ESPERA_CLIP\":(np.min,np.max,np.average,np.median,np.mean,np.std)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por hora del día\n",
    "data.groupby(\"HORA_DIA\").agg({\"TIEMPO_ESPERA_CLIP\":(np.min,np.max,np.average,np.median,np.mean,np.std)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conventional way to import seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# allow plots to appear within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the relationship between the features and the response using scatterplots\n",
    "sns.pairplot(data, x_vars=['DIA_SEMANA','DIA_MES','HORA_DIA'], y_vars='TIEMPO_ESPERA_CLIP', height=8, aspect=0.6, kind='reg')\n",
    "sns.pairplot(data, x_vars=['DIA_SEMANA','DIA_MES','HORA_DIA'], y_vars='TIEMPO_TOTAL_ATENCION_CLIP', height=8, aspect=0.6, kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the relationship between the features and the response using scatterplots\n",
    "sns.barplot(data, x='CATCLI', y='TIEMPO_ESPERA_CLIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the relationship between the features and the response using scatterplots\n",
    "sns.barplot(data, x='CATCLI', y='TIEMPO_TOTAL_ATENCION_CLIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the relationship between the features and the response using scatterplots\n",
    "sns.barplot(data, x='TIEMPO_ESPERA_CLIP', y='SUCCOD', orient='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the relationship between the features and the response using scatterplots\n",
    "sns.barplot(data, x='TIEMPO_TOTAL_ATENCION_CLIP', y='SUCCOD', orient='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the relationship between the features and the response using scatterplots\n",
    "sns.pairplot(data, x_vars=['TIPO_OPERACION'], y_vars='TIEMPO_ESPERA_CLIP', height=7, aspect=2, kind='scatter')\n",
    "sns.pairplot(data, x_vars=['TIPO_OPERACION'], y_vars='TIEMPO_TOTAL_ATENCION_CLIP', height=7, aspect=2, kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codificación de datos categóricos\n",
    "Muchos algoritmos de aprendizaje automático son incapaces de procesar variables categóricas. Por ejemplo: Bajo, Medio, Alto.\n",
    "\n",
    "Por lo tanto, es importante codificar los datos en una forma adecuada para poder preprocesar estas variables. La codificación consiste en convertir todas las variables de entrada y salida en numéricas. De este modo, el modelo podrá comprender y extraer la información generando la salida deseada. Los datos categóricos varían en función del número de valores posibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe for the categorical variable: CATCLI\n",
    "catcli_dummies = pd.get_dummies(data.CATCLI)\n",
    "data = pd.concat([data, catcli_dummies], axis=1)\n",
    "data.drop('HH', inplace=True, axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Python list of feature names\n",
    "feature_cols = ['AA','KK','LL','MM','N','NN','PP','QQ','RR','DIA_MES','DIA_SEMANA','HORA_DIA']\n",
    "\n",
    "# use the list to select a subset of the original DataFrame\n",
    "X = data[feature_cols]\n",
    "\n",
    "# print the first 5 and last 5 rows\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the type and shape of X\n",
    "print(type(X))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a Series from the DataFrame\n",
    "y = data['TIEMPO_ESPERA_CLIP']\n",
    "\n",
    "# print the first 5 values\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the type and shape of y\n",
    "print(type(y))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Separando conjunto de entrenamiento\n",
    "En scikit-learn una división aleatoria en conjuntos de entrenamiento y de prueba puede ser rápidamente calculada con la función de ayuda *train_test_split*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default split is 75% for training and 25% for testing\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Regresión lineal en scikit-learn\n",
    "LinearRegression ajusta un modelo lineal con coeficientes w = (w1, ..., wp) para minimizar la suma residual de cuadrados entre los objetivos observados en el conjunto de datos y los objetivos predichos por la aproximación lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# instantiate\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# fit the model to the training data (learn the coefficients)\n",
    "linreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Interpretando los coeficientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the intercept and coefficients\n",
    "print(linreg.intercept_)\n",
    "print(linreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair the feature names with the coefficients\n",
    "list(zip(feature_cols, linreg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Haciendo predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the testing set\n",
    "y_pred = linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Métricas de desempeño utilizadas\n",
    "\n",
    "### Métricas de evaluación del modelo para la regresión\n",
    "Las métricas de evaluación para problemas de clasificación, como **precisión**, no son útiles para problemas de regresión. En cambio, necesitamos métricas de evaluación diseñadas para comparar valores continuos.\n",
    "\n",
    "Vamos a crear algunas predicciones numéricas de ejemplo y calcular **tres métricas de evaluación comunes** para problemas de regresión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# calculate MAE using scikit-learn\n",
    "print(f\"MAE: {metrics.mean_absolute_error(y_test, y_pred)}\")\n",
    "\n",
    "# calculate MSE using scikit-learn\n",
    "print(f\"MSE: {metrics.mean_squared_error(y_test, y_pred)}\")\n",
    "\n",
    "# calculate RMSE using scikit-learn\n",
    "print(f\"RMSE: {np.sqrt(metrics.mean_squared_error(y_test, y_pred))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KFold\n",
    "El método Kfold devuelve el orden de las muestras elegidas para los conjuntos de entrenamiento y test en cada pliegue. En un marco de datos pandas tenemos que usar la función *.iloc* para obtener las filas correctas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, cross_validate, cross_val_score\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "i = 1\n",
    "\n",
    "print(f\"************************************************************************\")\n",
    "for train_index, test_index in kf.split(X):\n",
    "    cv_X_train = X.iloc[train_index]\n",
    "    cv_X_test = X.iloc[test_index]\n",
    "    cv_y_train = y.iloc[train_index]\n",
    "    cv_y_test = y.iloc[test_index]\n",
    "    \n",
    "    #Train the model\n",
    "    linreg.fit(cv_X_train, cv_y_train) #Training the model\n",
    "    cv_y_pred = linreg.predict(cv_X_test)\n",
    "    print(f\"MAE for the fold no. {i} on the test set: {metrics.mean_absolute_error(cv_y_test, cv_y_pred)}\")\n",
    "    # calculate MSE using scikit-learn\n",
    "    print(f\"MSE for the fold no. {i} on the test set: {metrics.mean_squared_error(cv_y_test, cv_y_pred)}\")\n",
    "    # calculate RMSE using scikit-learn\n",
    "    print(f\"RMSE for the fold no. {i} on the test set: {np.sqrt(metrics.mean_squared_error(cv_y_test, cv_y_pred))}\")\n",
    "    print(f\"************************************************************************\")\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Descripción de los resultados obtenidos\n",
    "_TODO_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "- [Gomes, Dipta & Nabil, Rashidul & Nur, Kamruddin. (2020). Banking Queue Waiting Time Prediction based on Predicted Service Time using Support Vector Regression. 10.1109/ICCAKM46823.2020.9051490.](https://www.researchgate.net/publication/339228555_Banking_Queue_Waiting_Time_Prediction_based_on_Predicted_Service_Time_using_Support_Vector_Regression)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
